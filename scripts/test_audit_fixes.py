#!/usr/bin/env python3
"""
üåå SarlakBot v3.2.0 - Audit Fixes Test Script
Comprehensive testing of all audit fixes and improvements
"""

import asyncio
import os
import sys
from datetime import datetime
from typing import Any

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from src.database.connection import db_manager
from src.utils.error_handler import error_handler
from src.utils.i18n_system import Language, i18n_system
from src.utils.input_validator import input_validator
from src.utils.logging import get_logger, setup_logging
from src.utils.monitoring import system_monitor

logger = get_logger(__name__)


class AuditTestSuite:
    """
    üåå Audit Test Suite
    Comprehensive testing of all audit fixes
    """

    def __init__(self):
        self.logger = logger
        self.test_results = []
        self.passed_tests = 0
        self.failed_tests = 0

    async def run_all_tests(self) -> dict[str, Any]:
        """Run all audit tests"""
        try:
            self.logger.info("üß™ Starting comprehensive audit test suite...")

            # Test categories
            await self._test_error_handling()
            await self._test_input_validation()
            await self._test_i18n_system()
            await self._test_monitoring_system()
            await self._test_database_health()
            await self._test_conversation_flows()
            await self._test_security_features()

            # Generate test report
            report = self._generate_test_report()

            self.logger.info(
                f"‚úÖ Test suite completed: {self.passed_tests} passed, {self.failed_tests} failed"
            )
            return report

        except Exception as e:
            self.logger.error(f"‚ùå Test suite failed: {e}")
            return {"status": "failed", "error": str(e), "timestamp": datetime.now().isoformat()}

    async def _test_error_handling(self) -> None:
        """Test error handling system"""
        try:
            self.logger.info("üîç Testing error handling system...")

            # Test error handler initialization
            assert error_handler is not None, "Error handler not initialized"
            self._record_test("Error Handler Initialization", True)

            # Test error counting
            initial_count = error_handler.error_counts.get(12345, [])
            error_handler._check_error_limit(12345)
            assert len(error_handler.error_counts.get(12345, [])) > len(
                initial_count
            ), "Error counting not working"
            self._record_test("Error Counting", True)

            self.logger.info("‚úÖ Error handling tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Error handling tests failed: {e}")
            self._record_test("Error Handling", False, str(e))

    async def _test_input_validation(self) -> None:
        """Test input validation system"""
        try:
            self.logger.info("üîç Testing input validation system...")

            # Test sanitization
            test_input = "<script>alert('xss')</script>"
            sanitized = input_validator.sanitize_input(test_input)
            assert "<script>" not in sanitized, "XSS sanitization failed"
            self._record_test("Input Sanitization", True)

            # Test display name validation
            is_valid, error = input_validator.validate_display_name("ÿπŸÑ€å ÿ±ÿ∂ÿß")
            assert is_valid, f"Valid display name rejected: {error}"
            self._record_test("Display Name Validation", True)

            # Test nickname validation
            is_valid, error = input_validator.validate_nickname("ali123")
            assert is_valid, f"Valid nickname rejected: {error}"
            self._record_test("Nickname Validation", True)

            # Test invalid input rejection
            is_valid, error = input_validator.validate_display_name("")
            assert not is_valid, "Empty name should be rejected"
            self._record_test("Invalid Input Rejection", True)

            self.logger.info("‚úÖ Input validation tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Input validation tests failed: {e}")
            self._record_test("Input Validation", False, str(e))

    async def _test_i18n_system(self) -> None:
        """Test internationalization system"""
        try:
            self.logger.info("üîç Testing i18n system...")

            # Test Persian text
            persian_text = i18n_system.get_text("welcome", Language.PERSIAN)
            assert "ÿÆŸàÿ¥ ÿ¢ŸÖÿØ€åÿØ" in persian_text, "Persian translation not working"
            self._record_test("Persian Translation", True)

            # Test English text
            english_text = i18n_system.get_text("welcome", Language.ENGLISH)
            assert "Welcome" in english_text, "English translation not working"
            self._record_test("English Translation", True)

            # Test fallback
            fallback_text = i18n_system.get_text("nonexistent_key", Language.PERSIAN)
            assert fallback_text == "nonexistent_key", "Fallback not working"
            self._record_test("Translation Fallback", True)

            # Test language validation
            valid_lang = i18n_system.validate_language("fa")
            assert valid_lang == Language.PERSIAN, "Language validation failed"
            self._record_test("Language Validation", True)

            self.logger.info("‚úÖ i18n system tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå i18n system tests failed: {e}")
            self._record_test("i18n System", False, str(e))

    async def _test_monitoring_system(self) -> None:
        """Test monitoring system"""
        try:
            self.logger.info("üîç Testing monitoring system...")

            # Test metrics
            metrics = system_monitor.get_metrics()
            assert "uptime_seconds" in metrics, "Metrics missing uptime"
            self._record_test("System Metrics", True)

            # Test request counting
            initial_count = system_monitor.request_count
            system_monitor.increment_request_count()
            assert system_monitor.request_count > initial_count, "Request counting not working"
            self._record_test("Request Counting", True)

            # Test error counting
            initial_errors = system_monitor.error_count
            system_monitor.increment_error_count()
            assert system_monitor.error_count > initial_errors, "Error counting not working"
            self._record_test("Error Counting", True)

            self.logger.info("‚úÖ Monitoring system tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Monitoring system tests failed: {e}")
            self._record_test("Monitoring System", False, str(e))

    async def _test_database_health(self) -> None:
        """Test database health"""
        try:
            self.logger.info("üîç Testing database health...")

            # Test database connection
            health = await db_manager.health_check()
            assert "status" in health, "Database health check missing status"
            self._record_test("Database Health Check", True)

            # Test connection pool
            if health.get("status") == "healthy":
                pool_stats = health.get("pool_stats", {})
                assert "size" in pool_stats, "Pool stats missing size"
                self._record_test("Database Pool Stats", True)

            self.logger.info("‚úÖ Database health tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Database health tests failed: {e}")
            self._record_test("Database Health", False, str(e))

    async def _test_conversation_flows(self) -> None:
        """Test conversation flows"""
        try:
            self.logger.info("üîç Testing conversation flows...")

            # Test onboarding state management
            from src.services.user_profile_service import (
                Language,
                OnboardingState,
                user_profile_service,
            )

            # Create test state
            test_state = OnboardingState(
                user_id=999999, language=Language.PERSIAN, display_name="ÿ™ÿ≥ÿ™", nickname="test_user"
            )

            # Test state saving
            success = await user_profile_service.save_onboarding_state(test_state)
            assert success, "Failed to save onboarding state"
            self._record_test("Onboarding State Save", True)

            # Test state retrieval
            retrieved_state = await user_profile_service.get_onboarding_state(999999)
            assert retrieved_state is not None, "Failed to retrieve onboarding state"
            assert retrieved_state.user_id == 999999, "Retrieved state has wrong user_id"
            self._record_test("Onboarding State Retrieval", True)

            # Clean up test state
            await user_profile_service._cleanup_onboarding_state(999999)

            self.logger.info("‚úÖ Conversation flows tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Conversation flows tests failed: {e}")
            self._record_test("Conversation Flows", False, str(e))

    async def _test_security_features(self) -> None:
        """Test security features"""
        try:
            self.logger.info("üîç Testing security features...")

            # Test input sanitization
            malicious_input = "'; DROP TABLE users; --"
            sanitized = input_validator.sanitize_input(malicious_input)
            assert "DROP TABLE" not in sanitized, "SQL injection not sanitized"
            self._record_test("SQL Injection Protection", True)

            # Test XSS protection
            xss_input = "<script>alert('xss')</script>"
            sanitized = input_validator.sanitize_input(xss_input)
            assert "<script>" not in sanitized, "XSS not sanitized"
            self._record_test("XSS Protection", True)

            # Test callback data validation
            is_valid, error = input_validator.validate_callback_data("valid_callback")
            assert is_valid, f"Valid callback data rejected: {error}"
            self._record_test("Callback Data Validation", True)

            # Test invalid callback data
            is_valid, error = input_validator.validate_callback_data("invalid<script>")
            assert not is_valid, "Invalid callback data should be rejected"
            self._record_test("Invalid Callback Data Rejection", True)

            self.logger.info("‚úÖ Security features tests passed")

        except Exception as e:
            self.logger.error(f"‚ùå Security features tests failed: {e}")
            self._record_test("Security Features", False, str(e))

    def _record_test(self, test_name: str, passed: bool, error: str = None) -> None:
        """Record test result"""
        result = {
            "test_name": test_name,
            "passed": passed,
            "error": error,
            "timestamp": datetime.now().isoformat(),
        }
        self.test_results.append(result)

        if passed:
            self.passed_tests += 1
        else:
            self.failed_tests += 1

    def _generate_test_report(self) -> dict[str, Any]:
        """Generate comprehensive test report"""
        return {
            "status": "completed",
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.test_results),
                "passed_tests": self.passed_tests,
                "failed_tests": self.failed_tests,
                "success_rate": (
                    (self.passed_tests / len(self.test_results)) * 100 if self.test_results else 0
                ),
            },
            "test_results": self.test_results,
            "recommendations": self._generate_recommendations(),
        }

    def _generate_recommendations(self) -> list[str]:
        """Generate recommendations based on test results"""
        recommendations = []

        if self.failed_tests > 0:
            recommendations.append("Review failed tests and fix issues before deployment")

        if self.passed_tests / len(self.test_results) < 0.9:
            recommendations.append("Consider additional testing before production deployment")

        recommendations.append("Run load testing in staging environment")
        recommendations.append("Monitor system performance in production")
        recommendations.append("Set up alerting for critical system metrics")

        return recommendations


async def main():
    """Main test function"""
    try:
        # Setup logging
        setup_logging(log_level="INFO")

        # Initialize database
        await db_manager.initialize()

        # Run test suite
        test_suite = AuditTestSuite()
        report = await test_suite.run_all_tests()

        # Print results
        print("\n" + "=" * 50)
        print("üß™ AUDIT TEST SUITE RESULTS")
        print("=" * 50)
        print(f"üìä Total Tests: {report['summary']['total_tests']}")
        print(f"‚úÖ Passed: {report['summary']['passed_tests']}")
        print(f"‚ùå Failed: {report['summary']['failed_tests']}")
        print(f"üìà Success Rate: {report['summary']['success_rate']:.1f}%")
        print("=" * 50)

        if report["summary"]["failed_tests"] > 0:
            print("\n‚ùå FAILED TESTS:")
            for result in report["test_results"]:
                if not result["passed"]:
                    print(f"  ‚Ä¢ {result['test_name']}: {result['error']}")

        print("\nüí° RECOMMENDATIONS:")
        for rec in report["recommendations"]:
            print(f"  ‚Ä¢ {rec}")

        print("\n" + "=" * 50)

        # Close database
        await db_manager.close()

        return report["summary"]["success_rate"] >= 90

    except Exception as e:
        logger.error(f"‚ùå Test suite failed: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
